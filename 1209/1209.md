In the context of GLM(generalized linear modeling),**we no longer need to assume y to be normally distributed and continuous.**

Remember in linear regression we fit the model below to data
$y_i=\beta_0+\beta_1x_{1i}+...+\beta_kx_{ki}+\varepsilon_i,i=1,2...,n$
where $y_i in [-\infty,+\infty] & \beta_0+\beta_1x_{1i}+....\beta_kx_{ki}in [-\infty,+\infty]$


Some of you may wonder, why bother with glm( )? Why not just use lm( ) ? Similar to what we did in lecture 6, let’s perform a simulation experiment.

```r
set.seed(3759)
b0=0.2
b1=0.5
n=1000
x=runif(n, -1, 1)
S=1000
par.est=matrix(NA, nrow=S, ncol=4)
for(s in 1:S){
    y=rpois(n,exp(b0+b1*x))
    model_glm=glm(y ~ x, family="poisson")
    model_lm=lm(y ~ x)
    par.est[s, 1]=model_glm$coef[1]
    par.est[s, 2]=model_lm$coef[1]
    par.est[s, 3]=model_glm$coef[2]
    par.est[s, 4]=model_lm$coef[2]
}
```

####How can we obtain a measure similar to R2 (i.e., pseudo-R2)?


We can manually estimate the beta parameters by optimizing the log-likelihood function

$\large \Sigma^n_{i=1}log[f(y_i|\lambda_i=e^{\beta_0+\beta_1x_{1i}+...\beta_kx_{ki}})]$

Below is how you can do it in R using the MLE techniques learnt from lecture 5.
```r
library(AER)
data(RecreationDemand)
head(RecreationDemand)
rd_Pois=glm(trips ~ ., data=RecreationDemand, family=poisson) 
summary(rd_Pois)
#
library(arm)
attach(RecreationDemand)
#
SKI=ifelse(ski=="yes",1,0)
USERFEE=ifelse(userfee=="yes",1,0)
Poisloglikf=function(par){
           lik=0
           for(i in 1:nrow(RecreationDemand)){
               lam=exp(1*par[1]+quality[i]*par[2]+SKI[i]*par[3]+
               income[i]*par[4]+USERFEE[i]*par[5]+costC[i]*par[6]+
               costS[i]*par[7]+costH[i]*par[8])
               lik=lik+dpois(trips[i],lam,log=TRUE)
           }
           -lik
}
#
est=nlminb(c(1,rep(0.001,7)),Poisloglikf,control=list(trace=1))
```

###Negative binomial regression
雖然Poisson distribution很有名,不過他假設($Var(y_i)/E(y_i)=\lambda_i/\lambda_i=1$),在大部分的情況下都不成立

所以我們會使用**negative binomial distribution **


####Is the equi-dispersion assumption likely to hold here? Let’s do a formal statistical test.
```r
dispersiontest(rd_Pois)
```
p-value很小,所以poisson的假設不成立,我們需要使用negative binomial distribution


###Zero-inflation and zero-truncation
Zero-inflation:很多資料是0

####How are we doing in terms of capturing the number of zeros in the data?
When facing excess zeros in data, apply modified models such as a zero-inflated Poisson
$$
###zero-truncation


####There are no p-values readily available, how can you tell the statistical significance of those estimated betas?
使用 z value, p value=1-pnorm(z) 

##Quantile regression
quantile regression, which has gradually become an attractive alternative to the ordinary least squares (OLS) regression in lecture 6.

####Suppose we want to estimate the following model in R
$Q_{log(wage)}(\tau|x)=\beta_0+\beta_1experience+\beta_2experience^2+\beta_3educations+\varepsilon$
```r
library(quantreg)
data("CPS1988")
cps_f=log(wage) ~ experience + I(experience^2) + education
cps_lad=rq(cps_f, data=CPS1988)
cps_ols=lm(cps_f, data=CPS1988)
summary(cps_lad)
summary(cps_ols)
```
The default of rq() is τ=0.5,也就是看平均的資料,你也可以設定高一點看前面的資料,或設定低一點看後面的資料

LAD (“least absolute deviations”) regression try to minize  $\Sigma|y_i-\hat{y}_i|$

```r
cps_rqbig=rq(cps_f, tau=seq(0.05, 0.95, 0.05), data=CPS1988)
cps_rqbigs=summary(cps_rqbig)
plot(cps_rqbigs)

```

####quantile regression的意義是什麼
看指定quantile 的beta與ols的beta有什麼不一樣

####為什麼圖會有灰色區塊
95%的區間

####紅線(實線,虛線)的意義有是什麼
實現是ols的結果,虛線是ols2的95%區間


##Binary response models
###Logit and probit models
It is very, very common that data analysts need to deal with binary response variable. The response y can be either true/yes/success or false/no/failure, usually coded as 1 and 0. In many data mining problems, the target is a binary response such as
profit or loss, response greater or less

Since yi is either 0 or 1, we can model yi as a Bernoulli distribution such that
$f(y_i)=p^{y_i}_i(1-p_i)^{1-y_i}$
where $p_i=P(y_i=1)&E[y_i]=p_i$

```r
inv.logit=function(p){ 
return(exp(p)/(1+exp(p)))}
#
set.seed(32945)
b0=0.2
b1=0.5
n=1000
x=runif(n, -1, 1)
S=1000
par.est=matrix(NA, nrow=S, ncol=2)
for(s in 1:S){
y=rbinom(n, 1, inv.logit(b0+b1*x))
model_glm=glm(y ~ x, family=binomial(link=logit)) 
model_lm=lm(y ~ x)
par.est[s, 1]=model_glm$coef[2]
par.est[s, 2]=model_lm$coef[2]
}
dev.new(width=12, height=5)
par(mfrow=c(1,2))
hist(par.est[,1],main="Logistic Reg b1")
abline(v=b1,col='red',lwd=2)
hist(par.est[,2],main="Linear Reg b1",xlim=c(0,0.5))
abline(v=b1,col='red',lwd=2)
```

```r
ibrary(AER)
data("SwissLabor")
attach(SwissLabor)
plot(participation~age,ylevels=2:1)
plot(participation~education,ylevels=2:1)
#
swiss_logit=glm(participation~.+I(age^2),data=SwissLabor,
family=binomial(link="logit"))
swiss_probit=glm(participation~.+I(age^2),data=SwissLabor,
family=binomial(link="probit"))
#
swiss_logit0=update(swiss_logit,formula=.~1)
summary(swiss_logit0)
1-as.vector(logLik(swiss_logit)/logLik(swiss_logit0))
#
anova(swiss_logit0,swiss_logit,test="Chisq")
#
table(true=SwissLabor$participation,pred=round(fitted(swiss_logit)))
table(true=SwissLabor$participation,pred=round(fitted(swiss_probit)))
```
We can calculate the MacFadden’s $pseudo-R^2$

```r
swiss_logit0=update(swiss_logit,formula=.~1)
summary(swiss_logit0)
1- as.vector(logLik(swiss_logit)/logLik(swiss_logit0))
```

Actually, both the logit and probit assume symmetric responses. When **the ratio of 1-to-0 is highly asymmetric**, you should try `the cloglog` link too.
如果資料的比例很懸殊時,i.e. 一堆成功,少數失敗

```r
swiss_cloglog=glm(participation~.+I(age^2),data=SwissLabor,
family=binomial(link="cloglog"))
pred2=prediction(fitted(swiss_cloglog),SwissLabor$participation)
plot(performance(pred2,"tpr","fpr"),col='red',lty=2,lwd=2)
plot(performance(pred,"tpr","fpr"),add=T)
```

##作業
###自己用MLE寫出posion的regression
