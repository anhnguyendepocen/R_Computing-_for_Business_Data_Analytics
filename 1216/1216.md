```r
data("MurderRates")
murder_logit=glm(I(executions>0)~time+income+noncauc+lfp+southern,
data=MurderRates,family=binomial)
coeftest(murder_logit)
```

```r
I(executions>0)#如果true回傳1,else 回傳0
```

你看`coeftest(murder_logit)`會發現southernyes的標準差超大

###What does the warning message and the big standard error imply?
```r
murder_logit2=glm(I(executions>0) ~ time + income + noncauc + lfp + southern, 
data=MurderRates, family=binomial, control=list(epsilon=1e-15, maxit=50, trace=F))
coeftest(murder_logit2)
```

####What is happening to the standard error now?
結果更糟了,或許我們應該把southern從input移走

##The numerical issue arises because maximum likelihood estimates do not exist here. Instead,
樣本數太少(44筆),MLE可能會讓結果變得很糟

```r
table(I(MurderRates$excutions>0),MurderRates$southern)
```
從中可以看出一些端倪


##Beta regression
How should we perform regression analysis when the response variable y is bounded within [0, 1] or (0, 1)?

beta regression的限制是**0 < y <1**

###如果我的y有可能是0或是1該怎麼辦?
The beta random variable has a theoretical domain in (0, 1). What if the ratio or proportion y
displays 0/1?A useful transformation is (y⋅(n−1)+0.5)/n where n is the sample size

$E(y|\mu,\phi)$


```r
gy.logit2=betareg(yield ~ batch + temp | temp, data=GasolineYield )
summary(gy.logit2)
```
$\phi=\beta_0+\beta_1temp$

There are several alternative link functions available, including:

* probit $g(\mu)=\Phi^{-1}(\mu)$
* log-log $g(\mu)=-log(-log(\mu))$
* the complementary log-log $g(\mu)=log(-log(1-\mu))$
* Cauchy $g(\mu)=tan(\pi(\mu-0.5))$

其他常用的Ratio response models:
```r
 gy.probit=betareg(yield ~ batch + temp, data=GasolineYield, link="probit" )
 gy.loglog=betareg(yield ~ batch + temp, data=GasolineYield, link="loglog" )
 gy.cloglog=betareg(yield ~ batch + temp, data=GasolineYield, link="cloglog" ) 
 gy.cauchy=betareg(yield ~ batch + temp, data=GasolineYield, link="cauchit" )
```

####How do we select the best model? Let’s compute the Akaike information criterion (AIC). 

```r
 AIC(gy.logit, gy.logit2, gy.probit, gy.loglog, gy.cloglog, gy.cauchy)
```

#Pick the model with the **SMALLEST** AIC!
選AIC最小的,

###AIC的意義:
選最小AIC的model等同於選最高likelihoods的model **after penalizing the number of model parameters**
因為對任何model來說,liklihood會隨著參數的數目增加


## Multivariate data, analysis, and visualization
Multivariate data arise when people record the values of several outcomes/subjects of interest across a sample of units/objects. Multivariate data are ubiquitous and the examples include:

* Chest, waist, and hips measurements on a group of people
* The scores for different subjects achieved by students
* Carat weight, clarity, color, and cut (4C) for a bunch of diamonds

Take the last case of diamonds for example, three questions might be addressed:
####Could carat weight, clarity, color, and cut be summarized in some way by combining the four measurements into a single index/number?
以鑽石為例,我們有c1,..c4等鑽石的屬性,我們有興趣的是鑽石的品質,是否有辦法把這些屬性轉換成一個index?
using principal component analysis

####Are the subtypes/subgroups of diamonds within which individual diamonds are of similar 4Cs and between which 4Cs differ?
cluster analysis


##Covariances, correlations, and distances
###Covariances
The **covariance** of two random variables is a measure of their linear dependence.
$\sigma_{ij}=Cov(X_i,X_j)=E[(X_i-\mu_i)(X_j-\mu_j)]$

If Xi and Xj are independent of each other, their covariance is zero. **The converse is NOT true. If i=j, the covariance of the variable with itself is just its variance**(i.e.他們有非線性關係)

####Covariances example in R
```r
library(MVA)
data(USairpollution)
attach(USairpollution)
round(cov(USairpollution),3)
```
自己與自己的Covariances,就是變異數(Variance)

##correlation coefficient
$\large \rho_{ij}=\frac{\sigma_{ij}}{\sigma_i\sigma_j}$ where $\sigma_i=\sqrt{\sigma^2_i}$

####correlation coefficient example in R
```
round(cor(USairpollution),3)
```

###distance
distance between the units in the data. For two units i and j, what serves as a measure of distance between them, given the variable values for the two?
The most common measure is Euclidean distance
$\large d_{ij}=\sqrt{\Sigma^q_{k=1}(x_{ik}-x_{jk})^2}$

where $x_{ik}$ and $x_{jk}$ ,k =1,...,q are the variable values for units i and j, respectively.
```r
round(dist(scale(USairpollution, center=FALSE)), 3)
```
####center=f
$\large x_i=\frac{x_i}{\sigma_x}$

####center=T
$\large x_i=\frac{x_i-\bar{x}}{\sigma_x}$


##The multivariate normal density function
假設我們有d1,d2,d3的降雨量,我們可以假設三個都是常態分配,
但是這三個可能會有關聯,所以我們需要multivariate normal density function

The simplest version of the multivariate normal density is the bivariate normal density with q=2; this can be written as:

```r
mu1=0
mu2=0.5
sig1=0.5
sig2=2
rho=0 #the two variable are basic independent
x=seq(-3,3,0.01)
y=seq(-3,3,0.01)
#
bivariate=function(x,y){
          term1=1/(2*pi*sig1*sig2*sqrt(1-rho^2))
          term2=(x-mu1)^2/sig1^2
          term3=-(2*rho*(x-mu1)*(y-mu2))/(sig1*sig2)
          term4=(y-mu2)^2/sig2^2
          z=term2+term3+term4
          term5=term1*exp((-z/(2*(1-rho^2))))           
          return(term5)
}
z=outer(x,y,bivariate)
persp(x,y,z,main="Bivariate Normal Distribution",theta=55,phi=30,r=40,
d=0.1,expand=0.5,ltheta=90,lphi=180,shade=0.4,ticktype="detailed",nticks=5)
```


##Some visual library for  Multivariate data
```r
pairs(USairpollution, pch=".",cex=2)
library(scatterplot3d)
s3d=scatterplot3d(temp, wind, SO2, type= "h", highlight.3d=T, angle=55)
fit=lm(SO2~temp+wind)
s3d$plane3d(fit)

#老師推薦用這個
library(Rcmdr)
scatter3d(temp,wind,SO2)

```

##Principal component analysis
In the domain of optimization/analytics, the problem of having too many variables is known as the “curse of dimensionality”,, which brings us to principal component analysis (PCA), a super popular and powerful technique with a central aim – reducing the dimensionality of a multivariate data set, while accounting for as much of the original variation in the data set (or minimizing information loss in dimension-reduction).

The basic goal of PCA is to describe variation in a set of correlated variables x= $(x_1,...x_q)$
in terms of a new set of uncorrelated variables y= $(y_1,...y_q)$  ,each of which is a linear
combination of the x variables. The variables y= $(y_1,...y_q)$ are the principal components.

They are derived in decreasing order of “importance” in the sense that $y_1$ accounts for as much as possible of the variation in the original data among all linear combinations of x. The $y_2$ is chosen to account for as much as possible of the remaining variation, subject to being uncorrelated with $y_1$, and so on.

PCA is useful when:

* There are too many explanatory variables relative to the number of observations 
* The explanatory variables are highly correlated


####use PCA in r
```r
round(cor(USairpollution[,-1]),3)
usair_pca=princomp(scale(USairpollution[,-1]))
summary(usair_pca,loadings=TRUE)
```

我們關心的是loading,通常會找loading>0.5
以上面的程式碼為例, $x'_1=-0.612manu-0.578popul$

####how many principal components do we need? 
```r
plot(usair_pca$sdev^2,xlab="component number",ylab="eigenvalue",type='l')
abline(h=1,lty=2,col='red')
```

我們希望只留下eigenvalue可以大於1的component,所以以上面的程式碼為例,我們只會留下前三個component

我們不用自己算新的component
```r
#快速的方法
usair_pca$scores[,1]
#手動自己來
usair_pca$loadings[,1]%*%t(scale(USairpollution)[,-1])
```

####比較直接使用lm與先pca在lm
```
usair_reg1=lm(SO2~.,data=USairpollution)
usair_reg2=lm(SO2~usair_pca$scores[,1:3],data=USairpollution)
summary(usair_reg1)
summary(usair_reg2)
```

可以看出雖然`usair_reg2`的input比較少,不過結果也比較差(rsquare比較小),

Notice that, the components with small variance do not necessarily have small correlations with the response variable SO2. Why is that?

```r
usair_reg3=lm(SO2~usair_pca$scores[,c(1,4:6)],data=USairpollution)
summary(usair_reg3)
```
雖然component4~6的eigenvalue很小,可是他們跟結果可能有很高的關聯性。
結論不要隨便把eigenvalue很小的component丟掉,多嘗試不同的component組合,可能會有意想不到的結果。
