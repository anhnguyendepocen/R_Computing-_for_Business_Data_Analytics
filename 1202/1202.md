如果你直接將mom.work帶入lm,他會被視作一個連續變數
```r
lm(kid.score~mom.work)
#mon is treated as a continuous variable

```

不過mom.work的1,2,3,4其實代表work的四種類型,因此你不應該把mom.work視為一個連續變數,他應該要是一個類別變數,所以下列的寫法會比較適合:

```r
lm(kid.score~as.factor(mom.work))
#mon is treated as a nominal variable
kid.score=82+3.854*mom.work2+11.500*mom.work3+5.210*mom.work4
```

####How many dummy variables are included? 
3

####Which category is the base level?
mom.work=1

因為work2的p value很小,所以我們有時候可以直接忽略他。

###Consider a linear regression model with interactions:
```r
fit.2=lm(kid.score~mom.hs+mom.iq+mom.hs:mom.iq)
coef(fit.2)
confint(fit.2)
plot(mom.iq,kid.score,pch=20)
curve(cbind(1,1,x,1*x)%*%coef(fit.2),add=T,col="red") #mom.hs=1
curve(cbind(1,0,x,0*x)%*%coef(fit.2),add=T,col="green") #mom.hs=0
```
出來的回歸方程式應該會長這樣:
kid.score=b0+51.6*mom.hs+0.96*mom.iq-0.48*mom.hs*mom.iq

mom.hs對kid.score的影響是正向的,
mom.iq對kid.score的影響是正向的,


Synergistic (enhancing) interactions: Both variables affect y in the same direction, and together they produce an even stronger effect. The additive effects and the interactive effect are all of the same sign.

Buffering interactions: Two predictors have coefficients of opposite sign. One predictor weakens the effect of the other predictor. The interaction “buffers” the weakening. 

Interference (antagonistic) interactions: Both predictors have coefficients of the same sign in additive form, but the interaction has a coefficient of opposite sign.

####Which type of interaction does fit.2 reveal?
Interference,mom.hs對kid.score的影響是正向的,mom.iq對kid.score的影響是正向的,不過兩者的interaction是負向的



```r
x.new=data.frame(mom.hs=1,mom.iq=100)#如果mom.hs=1,mom.iq=100,kid.score會是多少?
predict(fit.1,x.new,interval="prediction",level=0.95)
#      fit      lwr      upr
# 88.07226 52.37369 123.7708
```

kid.score最可能是88.07,在95%信心區間的情況下,最高是123.7,最少是52.3


```r
sigma=sd(residuals(fit.1))
ytilda.fixed=coef(fit.1)[1]+coef(fit.1)[2]*1+coef(fit.1)[3]*100
ytilda=ytilda.fixed+rnorm(5000,0,sigma)
c(quantile(ytilda,0.025),quantile(ytilda,0.975))
```


##檢驗prediction的performance

* Mean error

###Below are a few tips for building regression models for prediction (Gelman & Hill, 2009)
1. Include all variables that you expect to be relevant and important.
2. Sometimes you can average or sum several inputs to create an index/score.
3. For inputs with large effects, consider including their interactions well.(p-value)
4. If a predictor variable is not statistically significant with the expected sign, keep it.
例如你收集了收入,智商,在學時間的資料,如果你發現智商的p-value>0.1 你還是可以留著
5. If a predictor variable is statistically significant with the expected sign, you NEED it.
例如你收集了收入,智商,在學時間的資料,如果你發現智商的p-value<0.01 你需要他!!
6. If a predictor variable is NOT statistically significant with the non-expected sign, drop it.
例如你收集了收入,智商,在學時間的資料,如果你發現智商的p-value>0.5 丟掉這個變數!!
7. If a predictor variable is statistically significant with the non-expected sign, think hard!
如果你覺得智商應該要與收入呈現正相關,可是出來的結果是負相關,而且p-value很小,那可能是資料有錯,也可能是你的假設(智商應該要與收入呈現正相關)有錯
8. DO NOT throw away observations (e.g., outliers) arbitrarily. You are losing information.

##Model diagnostics

1. Non-linearity of the (x1, x2,..., xk) – y relationships
有可能你的資料是非線性關係

2. Correlation of error terms 
如果error不是iid(ex:這次的錯誤與上次的錯誤有關係)

3. Collinearity

4. Outliers

5. High-leverage points

6. Non-constant variance of error terms 
當我們做線性迴歸時,我們假設$y=b_0+b_1x+\varepsilon$
7. Non-normal errors
8. Endogeneity
x跟$\varepsilon$不是iid

####Experiment 1: Heteroskedasticity
```r
set.seed(123)
b0=0.2
b1=0.5
 n=1000
x=runif(n, -1, 1)
S=1000
gamma=1.5
par.est=matrix(NA, nrow=S, ncol=2)
for(s in 1:S){
      y1=b0+b1*x+rnorm(n, 0, exp(x*gamma)) #error與x有關!!!
       model1=lm(y1~x)
       sigma=summary(model1)$sigma
       y2=b0+b1*x+rnorm(n, 0, sigma)
       model2=lm(y2~x)
       par.est[s, 1]=model1$coef[2]
       par.est[s, 2]=model2$coef[2] 
 }

dev.new(width=8, height=5)
plot(density(par.est[,2]),main="",lwd=2,xlab="")
lines(density(par.est[,1]),col= 'red', lwd=2, lty=2)
legend(0.12,3,c("Homoskedastic","Heteroskedastic"),
lty=c(1,2),lwd=c(2,2),bty="n",cex=1.1,col=c('black','red'))
mtext(expression(paste(hat(beta[1]))),side=1,line=2,cex=1.1)
abline(v=b1,col='green')
```
如果x跟$\varepsilon$不是iid,那回歸的結果就會比較差。

####Experiment 2: Non-normal errors
```r
library(VGAM)
set.seed(123)
b0=0.2
b1=0.5
n=1000
x=runif(n, -1, 1)
S=1000
par.est=matrix(NA, nrow=S, ncol=2)
for(s in 1:S){
       y1=b0+b1*x+rnorm(n, 0, 1)
       model1=lm(y1~x)
       y2=b0+b1*x+rlaplace(n, 0, 1)
       model2=lm(y2~x)
       par.est[s, 1]=model1$coef[2]
       par.est[s, 2]=model2$coef[2] 
}

dev.new(width=8, height=5)

plot(density(par.est[,1]),main="",lwd=2,xlab="")
lines(density(par.est[,2]),col= 'red', lwd=2, lty=2)
legend(0.3,6,c("Normal Errors","Laplace Errors"),
lty=c(1,2),lwd=c(2,2),bty="n",cex=1.1,col=c('black','red'))
mtext(expression(paste(hat(beta[1]))),side=1,line=2,cex=1.1)
abline(v=0.5,col='green')
```

如果$\varepsilon$不是normal distribution的話,回歸的結果會變差QQ

##Variable transformation
```r
fit.2=lm(kid.score ~ mom.hs + mom.iq + mom.hs:mom.iq)
summary(fit.2)
```
要注意,如果mom.hs=1的時候,kid.score不一定會比mom.hs=0的時候多51.2682

The coefficient on mom.hs is 51.3 – does this mean that kids with mothers who graduated from high school do, on average, 51.3 points better on their tests?
NO! The model includes an interaction term, and 51.3 is the predicted difference for kids that differ in mom.hs, among those with mom.iq=0. Since mom.iq can hardly be close to zero, the comparison at zero, and the coefficient 51.3 is meaningless.
We can simplify the interpretation by first subtracting the mean of each input variable:
```r
 c.mom.hs=mom.hs-mean(mom.hs)
 c.mom.iq=mom.iq-mean(mom.iq)
 fit.3=lm(kid.score ~ c.mom.hs + c.mom.iq + c.mom.hs:c.mom.iq)
```

##GLM
could be a non-negative or
strictly positive integer count, a ratio in [0, 1], a binary indicator (0/1), a categorical response (e.g., ratings, voting results, ethnic groups), or a continuous real number (e.g., time-to-event).
在lecture 6的時候我們的y是連續的(在負無限到正無限),在lecture7裏,y可以是布林值,正整數,類別..etc
￼￼


##問題
###why F-statistic越大越好
f檢定可以映射成p value,p value代表的就是極端值的機率,所以p-value越小,表示你估計的越正確。

###why r-square太大不好(ex:0.95)

###lecture6 page 8, `including j variables for j categories makes the regression model NON-identifiable`這句話的意思是什麼?
如果你的類別變數有五種情況的話,用(5-1)=4個變數來代表就好了,如果你用5個變數來代表的話,會有共線性的問題

###如何檢驗是否要共線性?(ex:數學成績與物理成績有很高的關聯性)
1. 跑關聯矩陣,數字越高越有關聯
2. 使用vif(variance inflation factor),大於10的話就是有共線性

