##下禮拜考試
elearn上有題目。

lecture6,page16,如果一個函數的積分包含正負的部分,則結果會是|正|-|負|

lecture6 page2,SE的beta0,beta1打反了。

\\(
\large SE(\hat{\beta}_0)^2=\sigma^2[\frac{1}{n}+\frac{\bar{x}^2}{\Sigma^n_{i=1}(x_i-\bar{x})^2}] \\
\large SE(\hat{\beta}_1)^2=\frac{\sigma^2}{\Sigma^n_{i=1}(x_i-\bar{x})^2} \\
\\)

如果你的樣本數太小的話可以使用`Nonparametric bootstrapping`
Nonparametric bootstrapping (i.e., sample with replacement) turns out to be an attractive option.
從現有的樣本中抽樣然後多做幾次去算。

信心指數95是說你的beta會有95%的機率介於(bet-x,beta+x)之間

\\(
y=\beta_0+\beta_1x+\varepsilon \\
\varepsilon~Normal(0,\sigma^2) \\
y \backsim Normal(\beta0+\beta1\lambda,\sigma^2)
\\)

使用MLE來做regression

```r
errorMLE=function(par){
         loglik=0
         for(i in 1:nrow(cars)){
             loglik=dnorm(dist[i],(par[1]+par[2]*speed[i]),par[3],log=T)+loglik
         }
         -loglik
}
```

```r
plot(L1,which=1)
```

產生一條smooth line

```r
L2=lm(dist~0+speed) #沒有常數項
```

如果你覺得你的model是:
\\(
dist_j=\beta_1speed_i+\beta_2speed_i+\varepsilon,i=1,2...50
\\)

```r
L3=lm(dist~0+speed+I(speed^2))
```

```r
curve(1.239*x+0.090*x^2,add=T,col='blue',lwd=2,lty=3) #使用curve來看哪個model最接近
```

###rsquare
可以解釋的資料比例,越接近一越好

###hypo
當你有標準差之後，你就可以列出\\(\beta_1\\)會在哪個區間之間,如果你的區間是正到正(ex:95%的信心指數,介於1.2~3),
則\\(H_0\\)會被拒絕,如果你的區間是介於負到正,\\(H_0\\)不會被拒絕

##Multiple linear regression
\\(
y_i=\beta_0+\beta_1x_{1j}+....\beta_kx_{kj}+\varepsilon_i,i=1,2...n
\\)

we assume:
\\(
y_i \backsim N(\beta_0+\beta_1x_{1j}+....\beta_kx_{kj}+\varepsilon_i,\sigma^2),i=1,2...n
\\)


###example
Consider a linear regression model with two predictors:
\\(
kid.score_i=\beta_0+\beta_1mom.hs_i+\beta2mom.iq_i+\varepsilon_i,i=1,2...n
\\)

```r
fit.1=lm(kid.score~mom.hs+mom.iq)
```

rsquare is approximately equal to the squared correlation between yi and fitted \\(\hat(y)_i\\)

How can we verify this in R?
```r
cor(kid.score,fitted(fit.1))^2
```

####F-statistic

Joint Hypothesis Testing of \\(\beta1,\beta2\\)

\\(
H_0: \beta_1=\beta_2=0 \\
H_0: \beta_1 \neq 0 \,beta_2 \neq 0 \\
\\)

##Monte-Carlo simulation of simple linear regression
```r
set.seed(123)
b0=0.2
b1=0.5
n=1000
x=runif(n, -1, 1)
S=500
par.est=matrix(NA, nrow=S, ncol=4)
for(s in 1:S){
    y=b0+b1*x+rnorm(n, 0, 1)
    model=lm(y ~ x)
    vcv=vcov(model)
    par.est[s, 1]=model$coef[1]
    par.est[s, 2]=model$coef[2]
    par.est[s, 3]=sqrt(diag(vcv)[1])
    par.est[s, 4]=sqrt(diag(vcv)[2])
}
dev.new(width=12, height=5)
par(mfrow=c(1,2))
hist(par.est[,1])
abline(v=b0,col='red',lwd=2)
hist(par.est[,2])
abline(v=b1,col='red',lwd=2)
```

##question

1. 記得看lecture6.R 用MLE來做regression的部分
done

2. summary這個函數顯示的東西的意思
3. why rsquare is approximately equal to the squared correlation between yi and fitted \\(\hat(y)_i\\)
